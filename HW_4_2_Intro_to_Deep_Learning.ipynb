{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbLHTNfSclli"
      },
      "source": [
        "## **Секція 1. Логістична регресія з нуля.**\n",
        "\n",
        "Будемо крок за кроком будувати модель лог регресії з нуля для передбачення, чи буде врожай більше за 80 яблук (задача подібна до лекційної, але на класифікацію).\n",
        "\n",
        "Давайте нагадаємо основні формули для логістичної регресії.\n",
        "\n",
        "#### Функція гіпотези - обчислення передбачення у логістичній регресії:\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\sigma(x W^T + b) = \\frac{1}{1 + e^{-(x W^T + b)}}\n",
        "$$\n",
        "\n",
        "Де:\n",
        "- $ \\hat{y} $ — це ймовірність \"позитивного\" класу.\n",
        "- $ x $ — це вектор (або матриця для набору прикладів) вхідних даних.\n",
        "- $ W $ — це вектор (або матриця) вагових коефіцієнтів моделі.\n",
        "- $ b $ — це зміщення (bias).\n",
        "- $ \\sigma(z) $ — це сигмоїдна функція активації.\n",
        "\n",
        "#### Як обчислюється сигмоїдна функція:\n",
        "\n",
        "Сигмоїдна функція $ \\sigma(z) $ має вигляд:\n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "Ця функція перетворює будь-яке дійсне значення $ z $ в інтервал від 0 до 1, що дозволяє інтерпретувати вихід як ймовірність для логістичної регресії.\n",
        "\n",
        "#### Формула функції втрат для логістичної регресії (бінарна крос-ентропія):\n",
        "\n",
        "Функція втрат крос-ентропії оцінює, наскільки добре модель передбачає класи, порівнюючи передбачені ймовірності $ \\hat{y} $ із справжніми мітками $ y $. Формула наступна:\n",
        "\n",
        "$$\n",
        "L(y, \\hat{y}) = - \\left[ y \\cdot \\log(\\hat{y}) + (1 - y) \\cdot \\log(1 - \\hat{y}) \\right]\n",
        "$$\n",
        "\n",
        "Де:\n",
        "- $ y $ — це справжнє значення (мітка класу, 0 або 1).\n",
        "- $ \\hat{y} $ — це передбачене значення (ймовірність).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtOYB-RHfc_r"
      },
      "source": [
        "### 1. Inputs and targets\n",
        "Тут вже наведений код для ініціювання набору даних в форматі numpy. Перетворіть `inputs`, `targets` на `torch` тензори. Виведіть результат на екран."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QLKZ77x4v_-v"
      },
      "outputs": [],
      "source": [
        "# Вхідні дані (temp, rainfall, humidity)\n",
        "inputs = np.array([[73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70]], dtype='float32')\n",
        "\n",
        "# Таргети (apples > 80)\n",
        "targets = np.array([[0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1]], dtype='float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KjoeaDrk6fO7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs: tensor([[ 73.,  67.,  43.],\n",
            "        [ 91.,  88.,  64.],\n",
            "        [ 87., 134.,  58.],\n",
            "        [102.,  43.,  37.],\n",
            "        [ 69.,  96.,  70.]])\n",
            "targets: tensor([[0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.]])\n"
          ]
        }
      ],
      "source": [
        "# Transform inputs and targets into tensors\n",
        "inputs = torch.from_numpy(inputs)\n",
        "targets = torch.from_numpy(targets)\n",
        "print(\"inputs:\", inputs)\n",
        "print(\"targets:\", targets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKzbJKfOgGV8"
      },
      "source": [
        "### 2. Weights and biases\n",
        "Ініціюйте ваги `w`, `b` для моделі логістичної регресії потрібної форми зважаючи на розмірності даних випадковими значеннями з нормального розподілу. Лишаю тут код для фіксації `random_seed`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aXhKw6Tdj1-d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x218a780f790>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.random.manual_seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "eApcB7eb6h9o"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "weights: tensor([[0.6614, 0.2669, 0.0617]], requires_grad=True)\n",
            "biases: tensor([0.6213], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "w = torch.randn(1, 3, requires_grad=True)\n",
        "b = torch.randn(1, requires_grad=True)\n",
        "print(\"weights:\", w)\n",
        "print(\"biases:\", b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYGxNGTaf5s6"
      },
      "source": [
        "### 3. model\n",
        "Напишіть функцію `model`, яка буде обчислювати функцію гіпотези в логістичній регресії і дозволяти робити передбачення на основі введеного рядка даних і коефіцієнтів в змінних `w`, `b`.\n",
        "\n",
        "  **Важливий момент**, що функція `model` робить обчислення на `torch.tensors`, тож для математичних обчислень використовуємо функціонал `torch`, наприклад:\n",
        "  - обчислення $e^x$: `torch.exp(x)`\n",
        "  - обчислення $log(x)$: `torch.log(x)`\n",
        "  - обчислення середнього значення вектору `x`: `torch.mean(x)`\n",
        "\n",
        "  Використайте функцію `model` для обчислення передбачень з поточними значеннями `w`, `b`.Виведіть результат обчислень на екран.\n",
        "\n",
        "  Проаналізуйте передбачення. Чи не викликають вони у вас підозр? І якщо викликають, то чим це може бути зумовлено?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\n",
        "\\hat{y} = \\sigma(x W^T + b) = \\frac{1}{1 + e^{-(x W^T + b)}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "pSz2j4Fh6jBv"
      },
      "outputs": [],
      "source": [
        "def model(x, w, b):\n",
        "    return 1 / (1 + torch.exp(-(x @ w.t() + b)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "predictions: tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]], grad_fn=<MulBackward0>)\n"
          ]
        }
      ],
      "source": [
        "preds = model(inputs, w, b)\n",
        "print(\"predictions:\", preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the current initial values of weights and biases, the model always returns 0, 1, or values very close to them. This is due to inappropriate values for `w` and `b`, which result in very large or very small values of `z`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2AGM0Mb2yHa"
      },
      "source": [
        "### 4. `binary_cross_entropy`\n",
        "Напишіть функцію `binary_cross_entropy`, яка приймає на вхід передбачення моделі `predicted_probs` та справжні мітки в даних `true_labels` і обчислює значення втрат (loss)  за формулою бінарної крос-ентропії для кожного екземпляра та вертає середні втрати по всьому набору даних.\n",
        "  Використайте функцію `binary_cross_entropy` для обчислення втрат для поточних передбачень моделі."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\n",
        "L(y, \\hat{y}) = - \\left[ y \\cdot \\log(\\hat{y}) + (1 - y) \\cdot \\log(1 - \\hat{y}) \\right]\n",
        "$$\n",
        "Де:\n",
        "- $ y $ — це справжнє значення (мітка класу, 0 або 1).\n",
        "- $ \\hat{y} $ — це передбачене значення (ймовірність)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1bWlovvx6kZS"
      },
      "outputs": [],
      "source": [
        "def binary_cross_entropy(predicted_proba, true_labels):\n",
        "    t1 = (true_labels * torch.log(predicted_proba))\n",
        "    t2 = ((1 - true_labels) * torch.log(1 - predicted_proba))\n",
        "    return torch.mean(-(t1 + t2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(nan, grad_fn=<MeanBackward0>)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss = binary_cross_entropy(preds, targets)\n",
        "loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We obtained an infinite loss value because log(0) is undefined (approaches negative infinity for values very close to 0). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFKpQxdHi1__"
      },
      "source": [
        "### 5. backward and gradients\n",
        "Зробіть зворотнє поширення помилки і виведіть градієнти за параметрами `w`, `b`. Проаналізуйте їх значення. Як гадаєте, чому вони саме такі?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "YAbXUNSJ6mCl"
      },
      "outputs": [],
      "source": [
        "loss.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "w tensor([[0.6614, 0.2669, 0.0617]], requires_grad=True)\n",
            "w.grad: tensor([[nan, nan, nan]])\n"
          ]
        }
      ],
      "source": [
        "print(\"w\", w)\n",
        "print(\"w.grad:\", w.grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "b tensor([0.6213], requires_grad=True)\n",
            "b.grad: tensor([nan])\n"
          ]
        }
      ],
      "source": [
        "print(\"b\", b)\n",
        "print(\"b.grad:\", b.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For such values of `z` gradients are very close to 0 or undefined."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDN1t1RujQsK"
      },
      "source": [
        "**Що сталось?**\n",
        "\n",
        "В цій задачі, коли ми ініціювали значення випадковими значеннями з нормального розподілу, насправді ці значення не були дуже гарними стартовими значеннями і привели до того, що градієнти стали дуже малими або навіть рівними нулю (це призводить до того, що градієнти \"зникають\"), і відповідно при оновленні ваг у нас не буде нічого змінюватись. Це називається `gradient vanishing`. Це відбувається через **насичення сигмоїдної функції активації.**\n",
        "\n",
        "У нашій задачі ми використовуємо сигмоїдну функцію активації, яка має такий вигляд:\n",
        "\n",
        "   $$\n",
        "   \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "   $$\n",
        "\n",
        "\n",
        "Коли значення $z$ дуже велике або дуже мале, сигмоїдна функція починає \"насичуватись\". Це означає, що для великих позитивних $z$ сигмоїда наближається до 1, а для великих негативних — до 0. В цих діапазонах градієнти починають стрімко зменшуватись і наближаються до нуля (бо градієнт - це похідна, похідна на проміжку функції, де вона паралельна осі ОХ, дорівнює 0), що робить оновлення ваг неможливим.\n",
        "\n",
        "![](https://editor.analyticsvidhya.com/uploads/27889vaegp.png)\n",
        "\n",
        "У логістичній регресії $ z = x \\cdot w + b $. Якщо ваги $w, b$ - великі, значення $z$ також буде великим, і сигмоїда перейде в насичену область, де градієнти дуже малі.\n",
        "\n",
        "Саме це сталося в нашій задачі, де великі випадкові значення ваг викликали насичення сигмоїдної функції. Це в свою чергу призводить до того, що під час зворотного поширення помилки (backpropagation) модель оновлює ваги дуже повільно або зовсім не оновлює. Це називається проблемою **зникнення градієнтів** (gradient vanishing problem).\n",
        "\n",
        "**Що ж робити?**\n",
        "Ініціювати ваги маленькими значеннями навколо нуля. Наприклад ми можемо просто в існуючій ініціалізації ваги розділити на 1000. Можна також використати інший спосіб ініціалізації вагів - інформація про це [тут](https://www.geeksforgeeks.org/initialize-weights-in-pytorch/).\n",
        "\n",
        "Як це робити - показую нижче. **Виконайте код та знову обчисліть передбачення, лосс і виведіть градієнти.**\n",
        "\n",
        "А я пишу пояснення, чому просто не зробити\n",
        "\n",
        "```\n",
        "w = torch.randn(1, 3, requires_grad=True)/1000\n",
        "b = torch.randn(1, requires_grad=True)/1000\n",
        "```\n",
        "\n",
        "Нам потрібно, аби тензори вагів були листовими (leaf tensors).\n",
        "\n",
        "1. **Що таке листовий тензор**\n",
        "Листовий тензор — це тензор, який був створений користувачем безпосередньо і з якого починається обчислювальний граф. Якщо такий тензор має `requires_grad=True`, PyTorch буде відслідковувати всі операції, виконані над ним, щоб правильно обчислювати градієнти під час навчання.\n",
        "\n",
        "2. **Чому ми використовуємо `w.data` замість звичайних операцій**\n",
        "Якщо ми просто виконали б операції, такі як `(w - 0.5) / 100`, ми б отримали **новий тензор**, який вже не був би листовим тензором, оскільки ці операції створюють **новий** тензор, а не модифікують існуючий.\n",
        "\n",
        "  Проте, щоб залишити наші тензори ваги `w` та зміщення `b` листовими і продовжити можливість відстеження градієнтів під час тренування, ми використовуємо атрибут `.data`. Цей атрибут дозволяє **виконувати операції in-place (прямо на існуючому тензорі)** без зміни самого об'єкта тензора. Отже, тензор залишається листовим, і PyTorch може коректно обчислювати його градієнти.\n",
        "\n",
        "3. **Чому важливо залишити тензор листовим**\n",
        "Якщо тензор більше не є листовим (наприклад, через проведення операцій, що створюють нові тензори), ви не зможете отримати градієнти за допомогою `w.grad` чи `b.grad` після виклику `loss.backward()`. Це може призвести до втрати можливості оновлення параметрів під час тренування моделі. В нашому випадку ми хочемо, щоб тензори `w` та `b` накопичували градієнти, тому вони повинні залишатись листовими.\n",
        "\n",
        "**Висновок:**\n",
        "Ми використовуємо `.data`, щоб виконати операції зміни значень на ваги і зміщення **in-place**, залишаючи їх листовими тензорами, які можуть накопичувати градієнти під час навчання. Це дозволяє коректно працювати механізму зворотного поширення помилки (backpropagation) і оновлювати ваги моделі."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOPSQyttpVjO"
      },
      "source": [
        "### 5.2 Initialize weights with `.data`\n",
        "Виконайте код та знову обчисліть передбачення, лосс і знайдіть градієнти та виведіть всі ці тензори на екран."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "-EBOJ3tsnRaD"
      },
      "outputs": [],
      "source": [
        "torch.random.manual_seed(1)\n",
        "w = torch.randn(1, 3, requires_grad=True)  # Листовий тензор\n",
        "b = torch.randn(1, requires_grad=True)     # Листовий тензор\n",
        "\n",
        "# in-place операції\n",
        "w.data = w.data / 1000\n",
        "b.data = b.data / 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "-JwXiSpX6orh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "w: tensor([[6.6135e-04, 2.6692e-04, 6.1677e-05]], requires_grad=True)\n",
            "b: tensor([0.0006], requires_grad=True)\n",
            "predictions: tensor([[0.5174],\n",
            "        [0.5220],\n",
            "        [0.5244],\n",
            "        [0.5204],\n",
            "        [0.5190]], grad_fn=<MulBackward0>)\n",
            "loss: tensor(0.6829, grad_fn=<MeanBackward0>)\n",
            "w.grad: tensor([[ -5.4417, -18.9853, -10.0682]])\n",
            "b.grad: tensor([-0.0794])\n"
          ]
        }
      ],
      "source": [
        "print(\"w:\", w)\n",
        "print(\"b:\", b)\n",
        "\n",
        "preds = model(inputs, w, b)\n",
        "print(\"predictions:\", preds)\n",
        "\n",
        "loss = binary_cross_entropy(preds, targets)\n",
        "print(\"loss:\", loss)\n",
        "\n",
        "loss.backward()\n",
        "\n",
        "print(\"w.grad:\", w.grad)\n",
        "print(\"b.grad:\", b.grad)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this case, the initial value of `w` and `b` are small, resulting in finite loss value and gradients. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCdi44IT334o"
      },
      "source": [
        "### 6. Gradient descent\n",
        "Напишіть алгоритм градієнтного спуску, який буде навчати модель з використанням написаних раніше функцій і виконуючи оновлення ваг. Алгоритм має включати наступні кроки:\n",
        "\n",
        "  1. Генерація прогнозів\n",
        "  2. Обчислення втрат\n",
        "  3. Обчислення градієнтів (gradients) loss-функції відносно ваг і зсувів\n",
        "  4. Налаштування ваг шляхом віднімання невеликої величини, пропорційної градієнту (`learning_rate` домножений на градієнт)\n",
        "  5. Скидання градієнтів на нуль\n",
        "\n",
        "Виконайте градієнтний спуск протягом 1000 епох, обчисліть фінальні передбачення і проаналізуйте, чи вони точні?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "w = torch.randn(1, 3, requires_grad=True)  # Листовий тензор\n",
        "b = torch.randn(1, requires_grad=True)     # Листовий тензор\n",
        "\n",
        "# in-place операції\n",
        "w.data = w.data / 1000\n",
        "b.data = b.data / 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "mObHPyE06qsO"
      },
      "outputs": [],
      "source": [
        "epochs = 1000\n",
        "learning_rate = 1e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "targets: tensor([[0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.]])\n",
            "final predictions: tensor([[0.5780],\n",
            "        [0.6675],\n",
            "        [0.9129],\n",
            "        [0.1615],\n",
            "        [0.8643]], grad_fn=<MulBackward0>)\n",
            "final loss: tensor(0.3360, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ],
      "source": [
        "for i in range(epochs):\n",
        "    preds = model(inputs, w, b)\n",
        "    loss = binary_cross_entropy(preds, targets)\n",
        "    loss.backward()\n",
        "    with torch.no_grad():\n",
        "        w -= w.grad * learning_rate\n",
        "        b -= b.grad * learning_rate\n",
        "        w.grad.zero_()\n",
        "        b.grad.zero_()\n",
        "\n",
        "print(\"targets:\", targets)\n",
        "preds = model(inputs, w, b)\n",
        "print(\"final predictions:\", preds)\n",
        "loss = binary_cross_entropy(preds, targets)\n",
        "print(\"final loss:\", loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After 1000 epochs we obtained more appropriate predictions and significantly lower loss value. However, our model still does not perform well, as at the first item is predicted incorrectly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuRhlyF9qAia"
      },
      "source": [
        "## **Секція 2. Створення лог регресії з використанням функціоналу `torch.nn`.**\n",
        "\n",
        "Давайте повторно реалізуємо ту ж модель, використовуючи деякі вбудовані функції та класи з PyTorch.\n",
        "\n",
        "Даних у нас буде побільше - тож, визначаємо нові масиви."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "IX8Bhm74rV4M"
      },
      "outputs": [],
      "source": [
        "# Вхідні дані (temp, rainfall, humidity)\n",
        "inputs = np.array([[73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70],\n",
        "                   [73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70],\n",
        "                   [73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70]], dtype='float32')\n",
        "\n",
        "# Таргети (apples > 80)\n",
        "targets = np.array([[0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1]], dtype='float32')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7X2dV30KtAPu"
      },
      "source": [
        "### 7. TensorDataset\n",
        "Завантажте вхідні дані та мітки в PyTorch тензори та з них створіть датасет, який поєднує вхідні дані з мітками, використовуючи клас `TensorDataset`. Виведіть перші 3 елементи в датасеті.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "chrvMfBs6vjo"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[ 73.,  67.,  43.],\n",
              "         [ 91.,  88.,  64.],\n",
              "         [ 87., 134.,  58.]]),\n",
              " tensor([[0.],\n",
              "         [1.],\n",
              "         [1.]]))"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs = torch.from_numpy(inputs)\n",
        "targets = torch.from_numpy(targets)\n",
        "\n",
        "train_ds = TensorDataset(inputs, targets)\n",
        "train_ds[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nMFaa8suOd3"
      },
      "source": [
        "### 8. DataLoader\n",
        "Визначте data loader з класом **DataLoader** для підготовленого датасету `train_ds`, встановіть розмір батчу на 5 та увімкніть перемішування даних для ефективного навчання моделі. Виведіть перший елемент в дата лоадері."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZCsRo5Mx6wEI"
      },
      "outputs": [],
      "source": [
        "batch_size = 5\n",
        "train_loader = DataLoader(train_ds, batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[tensor([[ 69.,  96.,  70.],\n",
              "         [ 91.,  88.,  64.],\n",
              "         [ 69.,  96.,  70.],\n",
              "         [ 87., 134.,  58.],\n",
              "         [ 73.,  67.,  43.]]),\n",
              " tensor([[1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [0.]])]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "next(iter(train_loader))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymcQOo_hum6I"
      },
      "source": [
        "### 9. Class LogReg\n",
        "Створіть клас `LogReg` для логістичної регресії, наслідуючи модуль `torch.nn.Module` за прикладом в лекції (в частині про FeedForward мережі).\n",
        "\n",
        "  У нас модель складається з лінійної комбінації вхідних значень і застосування функції сигмоїда. Тож, нейромережа буде складатись з лінійного шару `nn.Linear` і використання активації `nn.Sigmoid`. У створеному класі мають бути реалізовані методи `__init__` з ініціалізацією шарів і метод `forward` для виконання прямого проходу моделі через лінійний шар і функцію активації.\n",
        "\n",
        "  Створіть екземпляр класу `LogReg` в змінній `model`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "EyAwhTBW6xxz"
      },
      "outputs": [],
      "source": [
        "class LogReg(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(3, 1)\n",
        "        self.act = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        x = self.act(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LogReg(\n",
              "  (linear): Linear(in_features=3, out_features=1, bias=True)\n",
              "  (act): Sigmoid()\n",
              ")"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = LogReg()\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RflV7xeVyoJy"
      },
      "source": [
        "### 10. Stockastic Gradient Descent and binary_cross_entropy\n",
        "Задайте оптимізатор `Stockastic Gradient Descent` в змінній `opt` для навчання моделі логістичної регресії. А також визначіть в змінній `loss` функцію втрат `binary_cross_entropy` з модуля `torch.nn.functional` для обчислення втрат моделі. Обчисліть втрати для поточних передбачень і міток, а потім виведіть їх. Зробіть висновок, чи моделі вдалось навчитись?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3QCATPU_6yfa"
      },
      "outputs": [],
      "source": [
        "opt = torch.optim.SGD(model.parameters(), lr=1e-5)\n",
        "\n",
        "loss_fn = F.binary_cross_entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(9.2144, grad_fn=<BinaryCrossEntropyBackward0>)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss_fn(model(inputs), targets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ch-WrYnKzMzq"
      },
      "source": [
        "### 11. Train the model\n",
        "Візьміть з лекції функцію для тренування моделі з відстеженням значень втрат і навчіть щойно визначену модель на 1000 епохах. Виведіть після цього графік зміни loss, фінальні передбачення і значення таргетів."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fit_return_loss(num_epochs, model, loss_fn, opt, train_loader):\n",
        "    losses = []\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "\n",
        "        for xb, yb in train_loader:\n",
        "            pred = model(xb)\n",
        "\n",
        "            loss = loss_fn(pred, yb)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        losses.append(avg_loss)\n",
        "\n",
        "        if (epoch + 1) % 50 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
        "    return losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_epochs = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "cEHQH9qE626k"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [50/1000], Loss: 2.6488\n",
            "Epoch [100/1000], Loss: 1.3008\n",
            "Epoch [150/1000], Loss: 0.2830\n",
            "Epoch [200/1000], Loss: 0.1544\n",
            "Epoch [250/1000], Loss: 0.1515\n",
            "Epoch [300/1000], Loss: 0.1511\n",
            "Epoch [350/1000], Loss: 0.1513\n",
            "Epoch [400/1000], Loss: 0.1504\n",
            "Epoch [450/1000], Loss: 0.1500\n",
            "Epoch [500/1000], Loss: 0.1497\n",
            "Epoch [550/1000], Loss: 0.1499\n",
            "Epoch [600/1000], Loss: 0.1490\n",
            "Epoch [650/1000], Loss: 0.1486\n",
            "Epoch [700/1000], Loss: 0.1487\n",
            "Epoch [750/1000], Loss: 0.1483\n",
            "Epoch [800/1000], Loss: 0.1474\n",
            "Epoch [850/1000], Loss: 0.1473\n",
            "Epoch [900/1000], Loss: 0.1475\n",
            "Epoch [950/1000], Loss: 0.1466\n",
            "Epoch [1000/1000], Loss: 0.1462\n"
          ]
        }
      ],
      "source": [
        "losses = fit_return_loss(num_epochs, model, loss_fn, opt, train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(losses, color='tan')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.]], grad_fn=<RoundBackward0>)"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model(inputs).round()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.]])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model performs well, and 200 epochs are sufficient to achieve these results."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py312",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
